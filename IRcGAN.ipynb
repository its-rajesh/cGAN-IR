{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b818c3cb-8bb3-415d-8e2c-d95b6dcd0227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros, ones\n",
    "from numpy.random import randn, randint\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, LeakyReLU, Dropout, Concatenate, MaxPooling2D, BatchNormalization\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ec5db-d189-4073-ac49-69a9f5c9c3ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d2d40ade-6b5a-4b97-9b7d-203f68dc26ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    #target = layers.Input(shape=input_shape)\n",
    "    \n",
    "    #x = layers.Concatenate()([inputs, target])\n",
    "    \n",
    "    down1 = layers.Conv2D(64, (3, 3), strides=(3, 3), padding='same')(inputs)#(x)\n",
    "    down1 = layers.LeakyReLU()(down1)\n",
    "    \n",
    "    down2 = layers.Conv2D(32, (3, 3), strides=(3, 3), padding='same')(down1)\n",
    "    down2 = layers.BatchNormalization()(down2)\n",
    "    down2 = layers.LeakyReLU()(down2)\n",
    "    \n",
    "    down3 = layers.Conv2D(16, (3, 3), strides=(3, 3), padding='same')(down2)\n",
    "    down3 = layers.BatchNormalization()(down3)\n",
    "    down3 = layers.LeakyReLU()(down3)\n",
    "    \n",
    "    down4 = layers.Conv2D(1, (3, 3), strides=(3, 3), padding='same')(down3)\n",
    "    dense = layers.Flatten()(down4)\n",
    "    dense = layers.Dense(100, activation='sigmoid')(dense)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    dis_mod = tf.keras.Model(inputs, outputs, name=\"Discriminator\")\n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    dis_mod.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return dis_mod\n",
    "\n",
    "input_shape=(3, 110240, 1)\n",
    "test_discr = define_discriminator(input_shape)\n",
    "#print(test_discr.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088c816c-b05d-4393-8f73-6597089d9453",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6af673af-9b91-49cd-94ac-4becc0e248ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_shape):\n",
    "\n",
    "    #Downsampling block 1\n",
    "    x = Conv2D(16, (3, 3), padding = \"same\", dilation_rate=1)(input_shape)\n",
    "    x = Conv2D(16, (3, 3), padding = \"same\", dilation_rate=1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(negative_slope=0.3)(x)\n",
    "    x = MaxPooling2D((1, 2))(x)\n",
    "    ds1 = Dropout(0.1)(x)\n",
    "\n",
    "    #Downsampling block 2\n",
    "    x = Conv2D(32, (3, 3), padding = \"same\", dilation_rate=2)(ds1)\n",
    "    x = Conv2D(32, (3, 3), padding = \"same\", dilation_rate=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(negative_slope=0.3)(x)\n",
    "    x = MaxPooling2D((1, 2))(x)\n",
    "    ds2 = Dropout(0.1)(x)\n",
    "\n",
    "    #Downsampling block 3\n",
    "    x = Conv2D(64, (3, 3), padding = \"same\", dilation_rate=4)(ds2)\n",
    "    x = Conv2D(64, (3, 3), padding = \"same\", dilation_rate=4)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(negative_slope=0.3)(x)\n",
    "    x = MaxPooling2D((1, 2))(x)\n",
    "    ds3 = Dropout(0.1)(x)\n",
    "\n",
    "    #Downsampling block 4\n",
    "    x = Conv2D(128, (3, 3), padding = \"same\", dilation_rate=16)(ds3)\n",
    "    x = Conv2D(128, (3, 3), padding = \"same\", dilation_rate=16)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(negative_slope=0.3)(x)\n",
    "    x = MaxPooling2D((1, 2))(x)\n",
    "    ds4 = Dropout(0.1)(x)\n",
    "\n",
    "    return ds4, ds3, ds2, ds1\n",
    "\n",
    "def bottleneck(encoder_output):\n",
    "    # Bottleneck layer\n",
    "    x = Conv2D(256, (3, 3), padding = \"same\")(encoder_output)\n",
    "    x = Conv2D(256, (3, 3), padding = \"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    out = LeakyReLU(negative_slope=0.3)(x)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def decoder(bottleneck_output, ds4, ds3, ds2, ds1):\n",
    "\n",
    "    #Upsampling Block 4\n",
    "    x = Conv2DTranspose(128, (3, 3), (1, 1), padding = \"same\")(bottleneck_output)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(negative_slope=0.3)(x)\n",
    "    x = concatenate([x, ds4])\n",
    "\n",
    "    x = Conv2D(128, (3, 3), padding = \"same\")(x)\n",
    "    x = Conv2D(128, (3, 3), padding = \"same\")(x)\n",
    "    up4 = LeakyReLU(negative_slope=0.3)(x)\n",
    "\n",
    "    #Upsampling Block 3\n",
    "    x = Conv2DTranspose(64, (3, 3), (1, 2), padding = \"same\")(up4)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(negative_slope=0.3)(x)\n",
    "    x = concatenate([x, ds3])\n",
    "\n",
    "    x = Conv2D(64, (3, 3), padding = \"same\")(x)\n",
    "    x = Conv2D(64, (3, 3), padding = \"same\")(x)\n",
    "    up3 = LeakyReLU(negative_slope=0.3)(x)\n",
    "\n",
    "    #Upsampling Block 2\n",
    "    x = Conv2DTranspose(32, (3, 3), (1, 2), padding = \"same\")(up3)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(negative_slope=0.3)(x)\n",
    "    x = concatenate([x, ds2])\n",
    "\n",
    "    x = Conv2D(32, (3, 3), padding = \"same\")(x)\n",
    "    x = Conv2D(32, (3, 3), padding = \"same\")(x)\n",
    "    up2 = LeakyReLU(negative_slope=0.3)(x)\n",
    "\n",
    "    #Upsampling Block 1\n",
    "    x = Conv2DTranspose(16, (3, 3), (1, 2), padding = \"same\")(up2)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(negative_slope=0.3)(x)\n",
    "    x = concatenate([x, ds1])\n",
    "\n",
    "    x = Conv2D(32, (3, 3), padding = \"same\")(x)\n",
    "    x = Conv2D(32, (3, 3), padding = \"same\")(x)\n",
    "    up1 = LeakyReLU(negative_slope=0.3)(x)\n",
    "\n",
    "    return up1\n",
    "\n",
    "def define_generator(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    ds4, ds3, ds2, ds1 = encoder(inputs)\n",
    "    bottleneck_output = bottleneck(ds4)\n",
    "    decoder_output = decoder(bottleneck_output, ds4, ds3, ds2, ds1)\n",
    "    \n",
    "    x = Conv2DTranspose(1, (3, 3), (1, 2), padding=\"same\")(decoder_output)\n",
    "    outputs = LeakyReLU(negative_slope=0.3)(x)\n",
    "    \n",
    "    \n",
    "    generator = Model(inputs, outputs, name=\"Generator\")\n",
    "    return generator\n",
    "\n",
    "# Example usage:\n",
    "input_shape = (3, 110240, 1)\n",
    "test_gen = define_generator(input_shape)\n",
    "#test_gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d874bf-deaa-48c6-a3f8-c6adc9411c30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83bceea6-6796-4b5b-80d2-62edfc76cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(g_model, d_model):\n",
    "    d_model.trainable = False\n",
    "    \n",
    "    gen_inp = g_model.input  \n",
    "    gen_output = g_model.output \n",
    "    \n",
    "    gan_output = d_model(gen_output)\n",
    "\n",
    "    model = Model(gen_inp, gan_output)\n",
    "\n",
    "    opt = Adam(learning_rate = 0.0002, beta_1 = 0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882afd18-5268-4458-9326-eeaf97552e9d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Useful Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "edfd3c22-7bc1-482e-a7df-8127b5f8483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples):\n",
    "    \n",
    "    Xtrain, Ytrain = dataset\n",
    "    ix = randint(0, Xtrain.shape[0], n_samples)\n",
    "    XTRAIN, YTRAIN = Xtrain[ix], Ytrain[ix]\n",
    "    \n",
    "    # generate class labels and assign to y (don't confuse this with the above labels that correspond to cifar labels)\n",
    "    y = ones((n_samples,))  #Label=1 indicating they are real\n",
    "    return [XTRAIN, YTRAIN], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "432e5c37-c141-45bc-aab7-ffab6edbcdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_samples(generator, XTRAIN, n_samples):\n",
    "    # predict outputs\n",
    "    Y_ESTIMATED = generator.predict(XTRAIN)\n",
    "    # create class labels\n",
    "    y = zeros((n_samples,))  #Label=0 indicating they are fake\n",
    "    return Y_ESTIMATED, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f8ca16-80b3-4b36-be8d-db251857ae0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885eef61-5393-4208-b3bc-29c9e4c1c015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce74bb7e-a1fd-4890-8921-13433d7d7c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385202a5-f116-4bb6-9abb-429a76f24139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a86e1c9-1350-4273-b3e7-a407d7729f98",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bda708d9-c144-4c7c-be68-9411717de8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g_model, d_model, gan_model, dataset, n_epochs, n_batch):\n",
    "    bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)  #the discriminator model is updated for a half batch of real samples \n",
    "                                   #and a half batch of fake samples, combined a single batch. \n",
    "    \n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        # enumerate batches over the training set\n",
    "        for j in range(bat_per_epo):\n",
    "\n",
    "            # Train the discriminator on real and fake images, separately (half batch each)\n",
    "            # Research showed that separate training is more effective. \n",
    "            # get randomly selected 'real' samples\n",
    "            # get randomly selected 'real' samples\n",
    "            [X_real, Y_real], y_real = generate_real_samples(dataset, half_batch)\n",
    "            # labels_real = Y_real\n",
    "            \n",
    "            # update discriminator model weights\n",
    "            # train_on_batch allows you to update weights based on a collection \n",
    "            # of samples you provide\n",
    "            d_loss_real, _ = d_model.train_on_batch(Y_real, y_real) #Training clean sources to discriminator.\n",
    "            \n",
    "            # generate 'fake' examples\n",
    "            X_fake, y_fake = generate_fake_samples(g_model, X_real, half_batch)\n",
    "            \n",
    "            # update discriminator model weights\n",
    "            d_loss_fake, _ = d_model.train_on_batch(X_fake, y_fake)\n",
    "            \n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            \n",
    "            #=====================================================================#\n",
    "            \n",
    "            [X_real, Y_real], y_real = generate_real_samples(dataset, n_batch)\n",
    "            # The generator wants the discriminator to label the generated samples\n",
    "            # as valid (ones)\n",
    "            # This is where the generator is trying to trick discriminator into believing\n",
    "            # the generated image is true (hence value of 1 for y)\n",
    "            # create inverted labels for the fake samples\n",
    "            y_gan = ones((n_batch, ))\n",
    "            # Generator is part of combined model where it got directly linked with the discriminator\n",
    "            # Train the generator with latent_dim as x and 1 as y. \n",
    "            # Again, 1 as the output as it is adversarial and if generator did a great\n",
    "            #job of folling the discriminator then the output would be 1 (true)\n",
    "            # update the generator via the discriminator's error\n",
    "            g_loss = gan_model.train_on_batch(X_real, y_gan)\n",
    "            \n",
    "            # Print losses on this batch\n",
    "            print('Epoch:{}, Batch:{}, Batch_per_epo:{}, d1:{}, d2={}, d_avg={}\\ng={},{},{}'.format(\n",
    "                i+1, j+1, bat_per_epo, d_loss_real, d_loss_fake, d_loss, g_loss[0], g_loss[1], g_loss[2]))\n",
    "            \n",
    "    # save the generator model\n",
    "    g_model.save('cGANIR.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cefe1e9b-ad77-459e-8ed8-63db0619d786",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = define_discriminator(input_shape)\n",
    "g_model = define_generator(input_shape)\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "\n",
    "Xtrain = np.random.random((10, 3, 110240, 1))\n",
    "Ytrain = np.random.random((10, 3, 110240, 1))\n",
    "dataset = np.array([Xtrain, Ytrain])\n",
    "\n",
    "train(g_model, d_model, gan_model, dataset, n_epochs=2, n_batch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069f21ec-268f-4451-9dc0-7a28cd1d0eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f88f57-9c05-4986-a9c5-15e0c0118e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d69372b-27f9-4990-a2cd-90b773a80581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b82de95-31fc-4135-bb10-5210e3facf63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e54e92f2-49f0-4123-ba9a-0a3b36c21062",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "59796178-0361-4641-aa52-599c18f843df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "# \n",
    "\n",
    "#Note: CIFAR10 classes are: airplane, automobile, bird, cat, deer, dog, frog, horse,\n",
    "# ship, truck\n",
    "\n",
    "# load model\n",
    "model = load_model('/Users/rajeshr/Desktop/Research2.0/ICASSP/cifar_conditional_generator_25epochs.h5')\n",
    "\n",
    "# Compile the model manually\n",
    "opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "X  = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46d6cbe-b7b1-47be-a092-053db8bcfe16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e368967c-317f-4d69-aadc-6868844fa0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10550bf9-fcae-4e38-9e80-8c40752f705d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a02ce4b-ceab-489a-9cd4-95b2b0699cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd76495-ef3b-4047-babd-a0f1f53b757f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa6b15-6bf9-45b5-858f-194556b91dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11130aea-455e-4728-9204-0e97335074b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1a174b-a959-4145-88f9-c524be563056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a1c72-61c2-4d32-a778-1bc705b72eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36773e69-51ef-477b-97de-78995520b7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6debb2a3-d145-40b5-9b91-ddaa13ab9b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51083a1b-5810-40ba-b70a-c931dbd8a2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
